import os
import glob
import streamlit as st                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        

from dotenv import load_dotenv
load_dotenv()
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# -----------------------------
# Streamlit basic config
# -----------------------------
st.set_page_config(page_title="ì›Œí™€ RAG ì±—ë´‡", page_icon="ğŸŒ", layout="wide")
st.title("ğŸŒ ì›Œí™€ ì±—ë´‡")
st.caption("ì›Œí‚¹í™€ë¦¬ë°ì´ ì¸í¬ì„¼í„° ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ì°¸ê³ í•œ íŒŒì¼ì„ í•¨ê»˜ í‘œì‹œí•©ë‹ˆë‹¤.")

BASE_DATA_DIR = "data"
COMMON_DIR = os.path.join(BASE_DATA_DIR, "common")

COUNTRY_MAP = {
    "ğŸ‡¦ğŸ‡º í˜¸ì£¼": "australia",
    "ğŸ‡¨ğŸ‡¦ ìºë‚˜ë‹¤": "canada",
    "ğŸ‡¯ğŸ‡µ ì¼ë³¸": "japan",
    "ğŸ‡³ğŸ‡¿ ë‰´ì§ˆëœë“œ": "newzealand",
    "ğŸ‡©ğŸ‡ª ë…ì¼": "germany"
}

# -----------------------------
# Helpers
# -----------------------------
def list_txt_files(country_dir: str) -> list[str]:
    return sorted(glob.glob(os.path.join(country_dir, "*.txt")))

def load_all_documents(base_dir: str) -> list[Document]:
    """
    data/ ì•„ë˜ì˜ common + ëª¨ë“  êµ­ê°€ í´ë”ì˜ TXTë¥¼ ì „ë¶€ ë¡œë”©
    """
    docs = []

    for root, dirs, files in os.walk(base_dir):
        for fname in files:
            if not fname.endswith(".txt"):
                continue

            fp = os.path.join(root, fname)
            with open(fp, "r", encoding="utf-8") as f:
                text = f.read().strip()

            # country ì¶”ì¶œ (data/japan/xxx.txt â†’ japan)
            parts = os.path.normpath(fp).split(os.sep)
            country = parts[1] if len(parts) > 1 else "unknown"

            docs.append(
                Document(
                    page_content=text,
                    metadata={
                        "source_file": fname,
                        "country": country,  # ğŸ”¥ í•µì‹¬
                        "full_path": fp,
                    },
                )
            )

    return docs


def format_context(docs: list[Document]) -> str:
    # LLMì— ê·¼ê±°ë¥¼ ëª…í™•íˆ ì£¼ê¸° ìœ„í•´ "íŒŒì¼ëª…"ì„ ê°™ì´ ë¶™ì„
    blocks = []
    for i, d in enumerate(docs, 1):
        src = d.metadata.get("source_file", "unknown.txt")
        blocks.append(f"[ê·¼ê±°{i} | {src}]\n{d.page_content}")
    return "\n\n---\n\n".join(blocks)

SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì›Œí‚¹í™€ë¦¬ë°ì´ ì¤€ë¹„ë¥¼ ë•ëŠ” ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.

ê¸°ë³¸ ê·œì¹™:
- ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ(Context)ì— ê¸°ë°˜í•´ì„œ ì‚¬ì‹¤ ì •ë³´ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.

ë¹„êµ/ì„ íƒ ì§ˆë¬¸ì— ëŒ€í•œ ì¶”ê°€ ê·œì¹™:
- "ì–´ë””ê°€ ì¢‹ì•„?", "ì¶”ì²œí•´ì¤˜", "ë¹„êµí•´ì¤˜"ì™€ ê°™ì€ ì§ˆë¬¸ì¼ ê²½ìš°
  í•˜ë‚˜ì˜ ì •ë‹µì„ ë‹¨ì •í•˜ì§€ ë§ê³ ,
  ì œê³µëœ ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•´
  ê° êµ­ê°€ì˜ íŠ¹ì§•ì„ í•­ëª©ë³„ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
- ê°œì¸ì˜ ì„±í–¥ì— ë”°ë¼ ì„ íƒì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒì„ ëª…í™•íˆ ë°íˆì„¸ìš”.
- ë§ˆì§€ë§‰ì— ì‚¬ìš©ìê°€ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ ê¸°ì¤€ì„ ì œì‹œí•˜ì„¸ìš”.

ì¶œë ¥ í˜•ì‹:
- í‘œ ë˜ëŠ” í•­ëª©ë³„ ë¹„êµë¥¼ ìš°ì„  ì‚¬ìš©í•˜ì„¸ìš”.
- í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
"""


def get_llm():
    return ChatOpenAI(model="gpt-4o-mini", temperature=0.2)

def get_embeddings():
    return OpenAIEmbeddings(model="text-embedding-3-small")

@st.cache_resource
def build_global_vectorstore():
    docs = load_all_documents(BASE_DATA_DIR)

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=150,
        separators=["\n\n", "\n", "-", "â€¢", " ", ""],
    )
    chunks = splitter.split_documents(docs)

    return FAISS.from_documents(chunks, get_embeddings())


# -----------------------------
# Sidebar UI
# -----------------------------
with st.sidebar:
    st.subheader("ì„¤ì •")
    country_label = st.selectbox("êµ­ê°€ ì„ íƒ", list(COUNTRY_MAP.keys()), index=0)
    country_key = COUNTRY_MAP[country_label]
    country_dir = os.path.join(BASE_DATA_DIR, country_key)

    k = st.slider("ê²€ìƒ‰ ë¬¸ì„œ ì¡°ê° ìˆ˜(top-k)", 2, 10, 5)
    show_context = st.checkbox("ê·¼ê±°(Context) ë³´ê¸°", value=False)

    st.markdown("---")
    # st.markdown("**ë°ì´í„° í´ë”**")
    # st.code(country_dir)
    # st.markdown("**TXT íŒŒì¼ ëª©ë¡**")
    # files = list_txt_files(country_dir)
    # if files:
    #     for f in files:
    #         st.write("-", os.path.basename(f))
    # else:
    #     st.write("ì—†ìŒ")

# -----------------------------
# Load global vectorstore
# -----------------------------
vectorstore = build_global_vectorstore()
retriever = vectorstore.as_retriever(search_kwargs={"k": k})
llm = get_llm()


# -----------------------------
# Chat state
# -----------------------------

# 1ï¸âƒ£ ìµœì´ˆ ì‹¤í–‰ ì‹œ ë©”ì‹œì§€ ì´ˆê¸°í™” (ë¬´ì¡°ê±´ ë¨¼ì €!)
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "assistant",
            "content": f"{country_label} ì›Œí™€ ê´€ë ¨í•´ì„œ ê¶ê¸ˆí•œ ê±° ë¬¼ì–´ë´! (ë¹„ì/ì •ì°©/ì·¨ì—…/ì•ˆì „/ê·€êµ­)"
        }
    ]

# 2ï¸âƒ£ êµ­ê°€ ë³€ê²½ ê°ì§€ìš©
if "prev_country" not in st.session_state:
    st.session_state.prev_country = country_label

# 3ï¸âƒ£ êµ­ê°€ ë³€ê²½ ê°ì§€
if st.session_state.prev_country != country_label:
    st.toast(
        f"{st.session_state.prev_country} â†’ {country_label}ë¡œ êµ­ê°€ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤",
        icon="â„¹ï¸"
    )

    ì•ˆë‚´ë©˜íŠ¸ = f"{country_label} ì›Œí™€ ê´€ë ¨í•´ì„œ ê¶ê¸ˆí•œ ê±° ë¬¼ì–´ë´! (ë¹„ì/ì •ì°©/ì·¨ì—…/ì•ˆì „/ê·€êµ­)"

    # Case 1: ì±„íŒ… ì—†ëŠ” ìƒíƒœ (ì´ˆê¸° ì•ˆë‚´ë§Œ ìˆìŒ)
    if (
        len(st.session_state.messages) == 1
        and st.session_state.messages[0]["role"] == "assistant"
    ):
        st.session_state.messages[0]["content"] = ì•ˆë‚´ë©˜íŠ¸

    # Case 2: ì´ë¯¸ ì±„íŒ… ì§„í–‰ ì¤‘
    else:
        st.session_state.messages.append(
            {
                "role": "assistant",
                "content": ì•ˆë‚´ë©˜íŠ¸
            }
        )

    st.session_state.prev_country = country_label


# êµ­ê°€ ë°”ê¾¸ë©´ ëŒ€í™” ì´ˆê¸°í™” ì˜µì…˜(ìë™ì€ X)
col1, col2 = st.columns([1, 1])
with col1:
    if st.button("ğŸ§¹ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = [
            {"role": "assistant", "content": f"{country_label} ì›Œí™€ ê´€ë ¨í•´ì„œ ê¶ê¸ˆí•œ ê±° ë¬¼ì–´ë´!"},
        ]
        st.rerun()

# render messages
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# -----------------------------
# Chat input
# -----------------------------
user_q = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì›Œí™€ ë¹„ì ì¡°ê±´ / ì„¸ê¸ˆ í™˜ê¸‰ / ê¸´ê¸‰ì „í™”)")
if user_q:
    st.session_state.messages.append({"role": "user", "content": user_q})
    with st.chat_message("user"):
        st.markdown(user_q)

    # retrieve
    docs = retriever.invoke(user_q)
    context = format_context(docs)

    final_prompt = f"""
{SYSTEM_PROMPT}

[Country]
{country_label}

[Context]
{context}

[User Question]
{user_q}
"""

    # generate
    with st.chat_message("assistant"):
        with st.spinner("ë¬¸ì„œì—ì„œ ê·¼ê±° ì°¾ëŠ” ì¤‘..."):
            resp = llm.invoke(final_prompt)
            answer = resp.content

        # used files
        used_files = []
        for d in docs:
            sf = d.metadata.get("source_file")
            if sf and sf not in used_files:
                used_files.append(sf)

        st.markdown(answer)
        st.markdown("---")
        st.markdown("### ğŸ“Œ ì°¸ê³  íŒŒì¼")
        st.write("ì¬ì™¸ë™í¬ì²­ ì›Œí‚¹í™€ë¦¬ë°ì´ ì¸í¬ì„¼í„°")

        if show_context:
            st.markdown("### ğŸ” ê·¼ê±°(Context)")
            st.code(context)

    st.session_state.messages.append({"role": "assistant", "content": answer})
