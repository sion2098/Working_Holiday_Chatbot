import os
import glob
import streamlit as st

from dotenv import load_dotenv
load_dotenv()
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# -----------------------------
# Streamlit Page Config
# -----------------------------
st.set_page_config(page_title="í˜¸ì£¼ ì›Œí™€ ì±—ë´‡ (ê³µì‹ë¬¸ì„œ ê¸°ë°˜)", page_icon="ğŸ‡¦ğŸ‡º", layout="wide")
st.title("ğŸ‡¦ğŸ‡º í˜¸ì£¼ ì›Œí™€ ì±—ë´‡ (ê³µì‹ë¬¸ì„œ ê¸°ë°˜)")
st.caption("ì¶œì²˜: ì¬ì™¸ë™í¬ì²­ ì›Œí‚¹í™€ë¦¬ë°ì´ì¸í¬ì„¼í„° PDFë¥¼ TXTë¡œ ì •ë¦¬í•œ ë°ì´í„°")

DATA_DIR = "data/australia"
SOURCE_HOME = "https://whic.mofa.go.kr/whic/nation/info.jsp?boardNo=100002"

# -----------------------------
# Helper: Load TXT files
# -----------------------------
def load_txt_documents(data_dir: str) -> list[Document]:
    file_paths = sorted(glob.glob(os.path.join(data_dir, "*.txt")))
    docs: list[Document] = []

    if not file_paths:
        return docs

    for fp in file_paths:
        with open(fp, "r", encoding="utf-8") as f:
            text = f.read().strip()

        docs.append(
            Document(
                page_content=text,
                metadata={
                    "source_file": os.path.basename(fp),
                    "source_url": SOURCE_HOME,
                    "country": "Australia",
                },
            )
        )
    return docs

# -----------------------------
# Build VectorStore (FAISS)
# -----------------------------
@st.cache_resource
def build_vectorstore():
    docs = load_txt_documents(DATA_DIR)
    if not docs:
        return None, "âŒ data/australia í´ë”ì— .txt íŒŒì¼ì´ ì—†ì–´ìš”. íŒŒì¼ ê²½ë¡œ/ì´ë¦„ì„ í™•ì¸í•´ì¤˜!"

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=150,
        separators=["\n\n", "\n", "â€¢", "-", " ", ""],
    )
    chunks = splitter.split_documents(docs)

    # ì„ë² ë”©/LLMì€ OPENAI_API_KEY í•„ìš”
    if not os.getenv("OPENAI_API_KEY"):
        return None, "âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì—†ì–´ìš”. í‚¤ ì„¤ì • í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì¤˜!"

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vs = FAISS.from_documents(chunks, embeddings)
    return vs, None

# -----------------------------
# LLM
# -----------------------------
def get_llm():
    # í•„ìš”í•˜ë©´ ëª¨ë¸ ë³€ê²½ ê°€ëŠ¥
    return ChatOpenAI(model="gpt-4o-mini", temperature=0.2)

# -----------------------------
# Prompt (RAG)
# -----------------------------
SYSTEM_PROMPT = """
ë„ˆëŠ” 'í˜¸ì£¼ ì›Œí‚¹í™€ë¦¬ë°ì´' ì •ë³´ ë„ìš°ë¯¸ì•¼.
ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ(Context)ì— ê·¼ê±°í•´ì„œë§Œ ë‹µë³€í•´.
ëª¨ë¥´ë©´ "ë¬¸ì„œì— ê·¼ê±°ê°€ ë¶€ì¡±í•´ í™•ë‹µí•˜ê¸° ì–´ë µë‹¤"ë¼ê³  ë§í•´.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ, í•µì‹¬ì„ ë¨¼ì €, ê·¸ ë‹¤ìŒì— ìƒì„¸ë¥¼ bulletë¡œ ì •ë¦¬í•´.
ë§ˆì§€ë§‰ì— ì°¸ê³ í•œ source_file ëª©ë¡ê³¼ source_urlì„ í‘œê¸°í•´.
"""

def format_context(docs: list[Document]) -> str:
    # ë¬¸ì„œ ì¡°ê° + ì¶œì²˜ íŒŒì¼ëª… ë¶™ì—¬ì„œ LLMì—ê²Œ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œê³µ
    blocks = []
    for i, d in enumerate(docs, 1):
        src = d.metadata.get("source_file", "unknown.txt")
        blocks.append(f"[ë¬¸ì„œ{i} | {src}]\n{d.page_content}")
    return "\n\n---\n\n".join(blocks)

# -----------------------------
# Sidebar
# -----------------------------
with st.sidebar:
    st.subheader("ì„¤ì •")
    k = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ì¡°ê° ìˆ˜ (top-k)", 2, 8, 4)
    st.markdown("---")
    st.markdown("**ë°ì´í„° ìœ„ì¹˜**")
    st.code(DATA_DIR)
    st.markdown("**ê³µì‹ ì¶œì²˜(í™ˆ)**")
    st.write("ì¬ì™¸ë™í¬ì²­ ì›Œí‚¹í™€ë¦¬ë°ì´ì¸í¬ì„¼í„°")  # ë§í¬ ì—°ê²°í•˜ê³  ì‹¶ìŒ (SOURCE_HOME)
    st.markdown("---")
    st.markdown("**ì˜ˆì‹œ ì§ˆë¬¸**")
    st.write("- í˜¸ì£¼ ì›Œí™€ ë‚˜ì´ ì œí•œ ì•Œë ¤ì¤˜")
    st.write("- 2nd/3rd ë¹„ì ì—°ì¥ ì¡°ê±´ ë­ì•¼?")
    st.write("- TFNì€ ì™œ í•„ìš”í•˜ê³  ì–´ë–»ê²Œ ì‹ ì²­í•´?")
    st.write("- ê·€êµ­ ì „ì— ì„¸ê¸ˆ í™˜ê¸‰ì€ ì–¸ì œ í•´?")

# -----------------------------
# Initialize VectorStore
# -----------------------------
vectorstore, error_msg = build_vectorstore()
if error_msg:
    st.error(error_msg)
    st.stop()

retriever = vectorstore.as_retriever(search_kwargs={"k": k})
llm = get_llm()

# -----------------------------
# Chat State
# -----------------------------
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ì•ˆë…•! í˜¸ì£¼ ì›Œí™€(ë¹„ì/ì •ì°©/ì·¨ì—…/ê·€êµ­) ê´€ë ¨í•´ì„œ ê¶ê¸ˆí•œ ê±° ë¬¼ì–´ë´ ğŸ˜Š"}
    ]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# -----------------------------
# Chat Input
# -----------------------------
user_q = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 2ì°¨ ë¹„ì ì¡°ê±´ ì•Œë ¤ì¤˜)")
if user_q:
    st.session_state.messages.append({"role": "user", "content": user_q})
    with st.chat_message("user"):
        st.markdown(user_q)

    # Retrieve relevant chunks
    # docs = retriever.get_relevant_documents(user_q)
    docs = retriever.invoke(user_q)
    context = format_context(docs)

    # Make final prompt
    final_prompt = f"""
{SYSTEM_PROMPT}

[Context]
{context}

[User Question]
{user_q}
"""

    with st.chat_message("assistant"):
        with st.spinner("ë¬¸ì„œì—ì„œ ê·¼ê±° ì°¾ëŠ” ì¤‘..."):
            resp = llm.invoke(final_prompt)
            answer = resp.content

        # ì¶œì²˜(ì¤‘ë³µ ì œê±°)
        used_files = []
        for d in docs:
            sf = d.metadata.get("source_file")
            if sf and sf not in used_files:
                used_files.append(sf)

        # Answer + Sources
        st.markdown(answer)
        st.markdown("---")
        st.markdown("### ğŸ“Œ ì°¸ê³  ì¶œì²˜")
        st.write("ë¬¸ì„œ:", ", ".join(used_files) if used_files else "N/A")
        st.write("ê³µì‹ ë§í¬:", SOURCE_HOME)

    st.session_state.messages.append({"role": "assistant", "content": answer})
